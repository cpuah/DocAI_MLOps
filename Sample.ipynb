{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Document Intelligence Custom Template User Feedback Loop Experiment\n",
    "\n",
    "This notebook demonstrates how to implement a simple way to enable a user to draw over a document that may have been processed by Azure AI Document Intelligence to provide feedback on the quality of results by highlighting incorrect or missing information with corrections. \n",
    "\n",
    "The goal is to showcase how a feedback mechanism can be implemented to allow the developers of custom models in Azure AI Document Intelligence to collect feedback from users to improve the model with the ability to retrain.\n",
    "\n",
    "> **Note**: This notebook only showcases the potential user interaction. The outputs are created as the labels JSON schema used by the Azure AI Document Intelligence service. The actual feedback processing and retraining of the model is not implemented in this notebook.\n",
    "\n",
    "## Pre-requisites\n",
    "\n",
    "The notebook uses [Dev Containers](https://code.visualstudio.com/docs/remote/containers) to ensure that all the required dependencies are available in a consistent local development environment.\n",
    "\n",
    "The following are required to run this notebook:\n",
    "\n",
    "- [Visual Studio Code](https://code.visualstudio.com/)\n",
    "- [Docker Desktop](https://www.docker.com/products/docker-desktop)\n",
    "- [Remote - Containers extension for Visual Studio Code](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers)\n",
    "\n",
    "> **Note**: The notebook is designed to run in a [Dev Container](https://code.visualstudio.com/docs/remote/containers) in Visual Studio Code. The Dev Container is pre-configured with the required dependencies and extensions. You can run this notebook outside of a Dev Container, but you will need to manually install the required dependencies including Poppler, Tesseract, and OpenCV.\n",
    "\n",
    "The Dev Container will include the following dependencies by default:\n",
    "\n",
    "- Debian 11 (Bullseye) base image\n",
    "- Python 3.12\n",
    "  - azure-ai-formrecognizer - for interacting with the Azure AI Document Intelligence service\n",
    "  - azure-core - for interacting with the Azure AI Document Intelligence service\n",
    "  - ipycanvas - for rendering the document and allowing the user to draw over it\n",
    "  - ipykernel - for running the notebook\n",
    "  - notebook - for running the notebook\n",
    "  - opencv-python-headless - for image processing\n",
    "  - pdf2image - for converting PDFs to images\n",
    "  - pytesseract - for performing OCR on the document\n",
    "- Poppler - used by pdf2image to convert PDFs to images\n",
    "- Tesseract OCR - used by pytesseract to perform OCR on the document\n",
    "- Python3 OpenCV - used for image processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Document Intelligence Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "from azure.ai.formrecognizer import (DocumentModelAdministrationClient, ModelBuildMode, DocumentAnalysisClient)\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.storage.blob import BlobServiceClient, ContainerSasPermissions, generate_container_sas\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "working_dir = os.path.abspath('')\n",
    "config = dotenv_values(f\"{working_dir}/config.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainingClient:\n",
    "    def __init__(self, config):\n",
    "        document_intelligence_endpoint = config['AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT']\n",
    "        document_intelligence_key = config['AZURE_DOCUMENT_INTELLIGENCE_KEY']\n",
    "        storage_connection_string = config['AZURE_STORAGE_ACCOUNT_CONNECTION_STRING']\n",
    "        training_data_container_name = config['AZURE_DOCUMENT_INTELLIGENCE_TRAINING_DATA_CONTAINER_NAME']\n",
    "\n",
    "        blob_service_client = BlobServiceClient.from_connection_string(storage_connection_string)\n",
    "\n",
    "        self.storage_account_key = config['AZURE_STORAGE_ACCOUNT_KEY']\n",
    "        self.training_data_container_client = blob_service_client.get_container_client(training_data_container_name)\n",
    "        self.document_model_admin_client = DocumentModelAdministrationClient(endpoint=document_intelligence_endpoint, credential=AzureKeyCredential(document_intelligence_key))\n",
    "        self.document_analysis_client = DocumentAnalysisClient(endpoint=document_intelligence_endpoint, credential=AzureKeyCredential(document_intelligence_key))\n",
    "\n",
    "    def upload_training_data(self, training_data_folder_path):\n",
    "        for root, _, files in os.walk(training_data_folder_path):\n",
    "            for file in files:\n",
    "                blob_client = self.training_data_container_client.get_blob_client(file)\n",
    "                with open(f\"{root}/{file}\", \"rb\") as data:\n",
    "                    blob_client.upload_blob(data, overwrite=True)\n",
    "\n",
    "        start_time = datetime.datetime.now(datetime.timezone.utc)\n",
    "        expiry_time = start_time + datetime.timedelta(days=1)\n",
    "\n",
    "        sas_token = generate_container_sas(\n",
    "            account_name=blob_client.account_name,\n",
    "            container_name=blob_client.container_name,\n",
    "            account_key=self.storage_account_key,\n",
    "            permission=ContainerSasPermissions(read=True, list=True),\n",
    "            expiry=expiry_time,\n",
    "            start=start_time\n",
    "        )\n",
    "\n",
    "        self.training_data_container_client_sas_url = f\"{self.training_data_container_client.url}?{sas_token}\"        \n",
    "    \n",
    "    def create_model(self, model_name):\n",
    "        try:\n",
    "            self.document_model_admin_client.delete_document_model(model_name)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        poller = self.document_model_admin_client.begin_build_document_model(\n",
    "            build_mode=ModelBuildMode.TEMPLATE,\n",
    "            blob_container_url=self.training_data_container_client_sas_url,\n",
    "            model_id=model_name\n",
    "        )\n",
    "        self.model = poller.result()\n",
    "        return self.model\n",
    "    \n",
    "    def run_layout_analysis(self, file_path):\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            poller = self.document_analysis_client.begin_analyze_document(model_id=self.model.model_id, document=f)\n",
    "            result = poller.result()\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training_client = ModelTrainingClient(config)\n",
    "model_training_client.upload_training_data(f\"{working_dir}/model_training\")\n",
    "invoice_model = model_training_client.create_model(\"invoice_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "The following will setup the required imports and constants for the notebook, including the path to the sample Invoice PDF that will be used, as well as creating the necessary paths to store the images and labels JSON outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "from ipycanvas import Canvas\n",
    "from ipywidgets import Image\n",
    "import pytesseract\n",
    "import cv2\n",
    "import json\n",
    "\n",
    "\n",
    "pdf_file_name = 'Invoice_6.pdf'\n",
    "pdf_dir = os.path.join(working_dir, 'pdfs')\n",
    "pdf_path = os.path.join(pdf_dir, pdf_file_name)\n",
    "\n",
    "images_dir = os.path.join(working_dir, 'images')\n",
    "if not os.path.exists(images_dir):\n",
    "    os.makedirs(images_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define object for tracking the user feedback options\n",
    "\n",
    "The following object is used to define the square border in which the user can draw over the document to provide feedback with. This object tracks the start and end coordinates of the border, as well as functions for performing the drawing of the border, normalizing the coordinates for the labels JSON output, and extracting the text within the border using OCR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquareBorder:\n",
    "    def __init__(self, image_path_ref: str, page_ref: int, border_width=2, border_color='black'):\n",
    "        self.image_path_ref = image_path_ref\n",
    "        self.page_ref = page_ref\n",
    "        self.border_width = border_width\n",
    "        self.border_color = border_color\n",
    "\n",
    "    def start(self, x, y):\n",
    "        self.start_x = x\n",
    "        self.start_y = y\n",
    "\n",
    "    def end(self, x, y):\n",
    "        self.end_x = x\n",
    "        self.end_y = y\n",
    "\n",
    "    def draw(self, canvas: Canvas):\n",
    "        canvas.stroke_style = self.border_color\n",
    "        canvas.line_width = self.border_width\n",
    "        canvas.stroke_rect(self.start_x, self.start_y, self.end_x - self.start_x, self.end_y - self.start_y)\n",
    "        self.normalize(canvas)\n",
    "\n",
    "    def normalize(self, canvas: Canvas):\n",
    "        # normalize the square_border pixels 0..1\n",
    "        self.start_x_normalized = self.start_x / canvas.width\n",
    "        self.start_y_normalized = self.start_y / canvas.height\n",
    "        self.end_x_normalized = self.end_x / canvas.width\n",
    "        self.end_y_normalized = self.end_y / canvas.height\n",
    "\n",
    "    def extract_text(self):\n",
    "        start_x_int = int(self.start_x)\n",
    "        start_y_int = int(self.start_y)\n",
    "        end_x_int = int(self.end_x)\n",
    "        end_y_int = int(self.end_y)\n",
    "\n",
    "        img = cv2.imread(self.image_path_ref)\n",
    "        crop_img = img[start_y_int:end_y_int, start_x_int:end_x_int]\n",
    "        self.text = pytesseract.image_to_string(crop_img)\n",
    "        return self.text\n",
    "\n",
    "    def get_bounding_box(self):\n",
    "        return [self.start_x_normalized, self.start_y_normalized, self.end_x_normalized, self.start_y_normalized, self.end_x_normalized, self.end_y_normalized, self.start_x_normalized, self.end_y_normalized]\n",
    "\n",
    "    def as_label(self):\n",
    "        return {\n",
    "            \"label\": \"\", \n",
    "            \"value\": [\n",
    "                {\n",
    "                    \"page\": self.page_ref,\n",
    "                    \"text\": self.extract_text(),\n",
    "                    \"boundingBoxes\": [self.get_bounding_box()]\n",
    "                }\n",
    "            ]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the PDF document into view for user feedback\n",
    "\n",
    "The following code will perform the following:\n",
    "\n",
    "1. Load the PDF document and store each page as an image using pdf2image.\n",
    "1. Display the rendered image using Canvas below as an interactive element in an output cell. **Note**: The image is rendered at the original size of the PDF page.\n",
    "1. Allow you to draw borders over the rendered image by clicking/holding, dragging, and releasing the mouse.\n",
    "\n",
    "> **Note**: This simple demonstration does not allow drawn borders to be removed or edited once drawn. To start again, you will need to re-run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "square_borders = []\n",
    "\n",
    "def handle_mouse_down_start_draw(canvas: Canvas, x, y):\n",
    "    square_border = SquareBorder(canvas.image_path_ref, canvas.page_ref)\n",
    "    square_border.start(x, y)\n",
    "    square_borders.append(square_border)\n",
    "\n",
    "def handle_mouse_down_end_draw(canvas: Canvas, x, y):\n",
    "    square_border = square_borders[-1]\n",
    "    square_border.end(x, y)\n",
    "    square_border.draw(canvas)\n",
    "\n",
    "def load_pdf(file_path: str):\n",
    "    pages = convert_from_path(file_path, fmt='jpeg')\n",
    "\n",
    "    print(f'Loaded {len(pages)} pages')\n",
    "\n",
    "    canvases = [Canvas(width=page.width, height=page.height) for page in pages]\n",
    "\n",
    "    for i, page in enumerate(pages):\n",
    "        page_ref = i + 1\n",
    "        image_path_ref = os.path.join(images_dir, f'{pdf_file_name}.page_{page_ref}.jpg')\n",
    "        page.save(image_path_ref, 'JPEG')\n",
    "\n",
    "        layout_analysis = model_training_client.run_layout_analysis(image_path_ref)\n",
    "        layout_analysis_path_ref = os.path.join(pdf_dir, f'{pdf_file_name}.ocr.json')\n",
    "        with open(layout_analysis_path_ref, 'w') as f:\n",
    "            json.dump(layout_analysis.to_dict(), f)\n",
    "    \n",
    "        canvases[i].image_path_ref = image_path_ref\n",
    "        canvases[i].page_ref = page_ref\n",
    "\n",
    "        canvases[i].draw_image(Image.from_file(image_path_ref), 0, 0, pages[i].width, pages[i].height)\n",
    "        canvases[i].on_mouse_down(lambda x, y: handle_mouse_down_start_draw(canvases[i], x, y))\n",
    "        canvases[i].on_mouse_up(lambda x, y: handle_mouse_down_end_draw(canvases[i], x, y))\n",
    "\n",
    "    return canvases\n",
    "\n",
    "canvases = load_pdf(pdf_path)\n",
    "\n",
    "canvases[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the user feedback into Document Intelligence labels format\n",
    "\n",
    "Once the user has drawn borders over the document to provide feedback, the following code will process the drawn borders into the labels JSON format used by the Azure AI Document Intelligence service. The files will be saved to the `./pdfs` directory with the name format `<pdf_file_name>.labels.json`.\n",
    "\n",
    "In a real-world scenario, the labels JSON files could be loaded into a UI to allow the user to update the label names associated with the custom model, and then retrain the model using the updated labels and PDF documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {\n",
    "    \"$schema\": \"https://schema.cognitiveservices.azure.com/formrecognizer/2021-03-01/labels.json\",\n",
    "    \"document\": pdf_file_name,\n",
    "    \"labels\": [square_border.as_label() for square_border in square_borders]\n",
    "}\n",
    "\n",
    "labels_file_path = os.path.join(pdf_dir, f'{pdf_file_name}.labels.json')\n",
    "with open(labels_file_path, 'w') as f:\n",
    "    json.dump(labels, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
