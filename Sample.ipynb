{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Document Intelligence Custom Template User Feedback Loop Experiment\n",
    "\n",
    "This experiment demonstrates how to replicate the functionality of the [Azure AI Document Intelligence](https://learn.microsoft.com/en-GB/azure/ai-services/document-intelligence/overview) Studio custom model training process to showcase how to create a user feedback loop for improving the quality of document processing results.\n",
    "\n",
    "This notebook showcases a more interactive user feedback experience, enabling a user to draw over an uploaded, analyzed document to provide feedback on the quality of results by highlighting incorrect or missing information with corrections. This implementation could be replicated in any client application using your chosen framework capabilities.\n",
    "\n",
    "The goal is to showcase how a feedback mechanism can be implemented to allow the developers of custom models in Azure AI Document Intelligence to collect feedback from users to improve the model with the ability to retrain.\n",
    "\n",
    "> **Note**: This notebook provides _one_ potential approach to user interaction, and can be interpreted in many ways based on your use case.\n",
    "\n",
    "## Pre-requisites\n",
    "\n",
    "> **Note**: Before continuing, please ensure that the [`Setup-Environment.ps1`](./Setup-Environment.ps1) script has been run to deploy the required infrastructure to Azure. This includes the Azure AI Document Intelligence resource and the Azure Storage account for creating a custom model.\n",
    "\n",
    "This notebook uses [Dev Containers](https://code.visualstudio.com/docs/remote/containers) to ensure that all the required dependencies are available in a consistent local development environment.\n",
    "\n",
    "The following are required to run this notebook:\n",
    "\n",
    "- [Visual Studio Code](https://code.visualstudio.com/)\n",
    "- [Docker Desktop](https://www.docker.com/products/docker-desktop)\n",
    "- [Remote - Containers extension for Visual Studio Code](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers)\n",
    "\n",
    "> **Note**: The Dev Container is pre-configured with the required dependencies and extensions. You can run this notebook outside of a Dev Container, but you will need to manually install the required dependencies including Poppler, Tesseract, and OpenCV.\n",
    "\n",
    "The Dev Container will include the following dependencies by default:\n",
    "\n",
    "- Debian 11 (Bullseye) base image\n",
    "- Python 3.12\n",
    "  - azure-ai-formrecognizer - for interacting with the Azure AI Document Intelligence service\n",
    "  - azure-core - for interacting with the Azure AI Document Intelligence service\n",
    "  - ipycanvas - for rendering the document and allowing the user to draw over it\n",
    "  - ipykernel - for running the notebook\n",
    "  - notebook - for running the notebook\n",
    "  - opencv-python-headless - for image processing\n",
    "  - pdf2image - for converting PDFs to images\n",
    "  - pytesseract - for performing OCR on the document\n",
    "- Poppler - used by pdf2image to convert PDFs to images\n",
    "- Tesseract OCR - used by pytesseract to perform OCR on the document\n",
    "- Python3 OpenCV - used for image processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Requirements\n",
    "\n",
    "The following code block imports the required dependencies for this notebook.\n",
    "\n",
    "It also configures the following:\n",
    "\n",
    "- Setup the local working directory.\n",
    "- Load local environment variables based on the output of the [`Setup-Environment.ps1`](./Setup-Environment.ps1) script run. The environment variables will be available in the [`.env`](./.env) file.\n",
    "- Initialize the credential that will be used to authentication with the Azure services.\n",
    "\n",
    "> **Note**: The [`Setup-Environment.ps1`](./Setup-Environment.ps1) script is not run as part of this notebook. It must be run separately, prior to running this notebook, to deploy the required infrastructure to Azure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "from dotenv import dotenv_values\n",
    "from ipywidgets import (VBox, Label)\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from modules.app_settings import AppSettings\n",
    "from modules.model_training_client import ModelTrainingClient\n",
    "from modules.document_canvas import (DocumentCanvas)\n",
    "from modules.document_intelligence_label import DocumentIntelligenceLabel\n",
    "from modules.document_intelligence_result_formatter import DocumentIntelligenceResultFormatter\n",
    "\n",
    "working_dir = os.path.abspath('')\n",
    "settings = AppSettings(dotenv_values(f\"{working_dir}/.env\"))\n",
    "azure_credential = DefaultAzureCredential(\n",
    "    exclude_environment_credential=True,\n",
    "    exclude_managed_identity_credential=True,\n",
    "    exclude_shared_token_cache_credential=True,\n",
    "    exclude_interactive_browser_credential=True,\n",
    "    exclude_powershell_credential=True,\n",
    "    exclude_visual_studio_code_credential=False,\n",
    "    exclude_cli_credential=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Document Intelligence Custom Model\n",
    "\n",
    "In order to improve a model, you need to have one. This experiment comes prepared with the data required to train a custom model. The data is located in the [`model_training`](./model_training/) directory and contains a set of invoices that will be used by the following steps.\n",
    "\n",
    "The steps below perform the following:\n",
    "\n",
    "- Configures the initial values that will represent the model naming convention.\n",
    "- Create a model training client (using the provided class) and run it to upload the files to Azure Blob Storage, and training the model using Azure AI Document Intelligence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The name of the model\n",
    "model_name = 'invoices' \n",
    "\n",
    "# The version of the model\n",
    "initial_model_version = '1.0.0'\n",
    "\n",
    "# The name of the model that will be registered in Azure AI Document Intelligence\n",
    "initial_model_id = f\"{model_name}-{initial_model_version}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training_client = ModelTrainingClient(settings=settings, use_azure_credential=False, azure_credential=azure_credential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resets the sample environment to only contain the initial training set. This is only necessary if the sample has been run previously.\n",
    "model_training_client.delete_training_data(\"6\")\n",
    "\n",
    "# Uploads the initial training set to Azure Blob Storage and initiates model training using the uploaded data.\n",
    "model_training_client.upload_training_data(f\"{working_dir}/model_training\")\n",
    "invoice_model = model_training_client.create_model(model_name=initial_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Feedback Loop\n",
    "\n",
    "The user feedback loop is a mechanism that allows users to provide feedback on the quality of the results generated by the model from interactions they have with it using their own data.\n",
    "\n",
    "This section emulates what a user experience flow may present itself within an intelligent application interfacing with Azure AI Document Intelligence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The name of the PDF file the user is providing.\n",
    "pdf_file_name = 'Invoice_6.pdf'\n",
    "\n",
    "# The directory containing the PDF file.\n",
    "pdf_dir = os.path.join(working_dir, 'pdfs')\n",
    "\n",
    "# The file path to the PDF file for loading.\n",
    "pdf_path = os.path.join(pdf_dir, pdf_file_name)\n",
    "\n",
    "# The file path to where the required JSON result from Azure AI Document Intelligence layout analysis will be stored.\n",
    "pdf_ocr_path = os.path.join(pdf_dir, f\"{pdf_file_name}.ocr.json\")\n",
    "\n",
    "# The file path to where the required JSON result for Azure AI Document Intelligence labels will be stored after user feedback.\n",
    "pdf_labels_path = os.path.join(pdf_dir, f\"{pdf_file_name}.labels.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run layout analysis on the PDF document using Azure AI Document Intelligence\n",
    "\n",
    "This step will use the Azure AI Document Intelligence service to perform layout analysis on the PDF document. When complete, the files will be saved to the `./pdfs` directory with the name format `<pdf_file_name>.ocr.json`.\n",
    "\n",
    "> **Note**: This specific step does not need to be run every time. The layout analysis is only required to be run once to capture the initial state of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training_client.run_layout_analysis(pdf_path, pdf_ocr_path, 'prebuilt-layout')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the PDF in the notebook for user feedback\n",
    "\n",
    "The following code will perform the following:\n",
    "\n",
    "1. Load the PDF document and store each page as an image using pdf2image.\n",
    "1. Display the rendered image using Canvas below as an interactive element in an output cell. **Note**: The image is rendered at the original size of the PDF page.\n",
    "1. Allow you to draw label regions over the rendered image by clicking/holding, dragging, and releasing the mouse.\n",
    "\n",
    "Below is an example of this interaction in action.\n",
    "\n",
    "![Demonstration of canvas selection](./media/canvas-selection.gif)\n",
    "\n",
    "As well as displaying the PDF, the required fields for the model are also displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields_file_path = os.path.join(working_dir, 'model_training', 'fields.json')\n",
    "with open(fields_file_path, 'r') as f:\n",
    "    fields = json.load(f)\n",
    "\n",
    "display_fields = [Label(value=f'{field['fieldKey']}') for field in fields['fields']]\n",
    "display_container = VBox(display_fields)\n",
    "\n",
    "print('Fields to capture')\n",
    "display(display_container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: This simple demonstration below to load the PDF to a canvas does not allow drawn regions to be removed or edited once drawn. To start again, you will need to re-run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_canvas = DocumentCanvas(working_dir)\n",
    "\n",
    "canvases = doc_canvas.load_pdf(pdf_path)\n",
    "for canvas in canvases:\n",
    "    display(canvas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the user feedback into Document Intelligence labels format\n",
    "\n",
    "Once the user has drawn borders over the document to provide feedback, the following code will process the drawn borders into the labels JSON format used by the Azure AI Document Intelligence service. The files will be saved to the `./pdfs` directory with the name format `<pdf_file_name>.labels.json`.\n",
    "\n",
    "In a real-world scenario, the labels JSON files could be loaded into a UI to allow the user to update the label names associated with the custom model, and then retrain the model using the updated labels and PDF documents. For the purposes of this experiment, these are rendered as UI inputs in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Render the user feedback as UI inputs\n",
    "\n",
    "The following code will render the user feedback as UI inputs in the notebook as output. You can update the label names associated with the custom model.\n",
    "\n",
    "Each square border previously drawn over the document will be rendered as a UI input in the notebook. The fields will be pre-populated, and you will be able to select the label from the available options for the model, and update the text associated with the label.\n",
    "\n",
    "> **Note**: If the label option is an array, you will be provided with additional options to provide the row number and the column label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [DocumentIntelligenceLabel(label_region, fields) for label_region in doc_canvas.label_regions]\n",
    "    \n",
    "for label in labels:\n",
    "    display(label.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the labels JSON file\n",
    "\n",
    "Once the user has updated the label names and text associated with the drawn borders, the following code will create the labels JSON file in the format required by the Azure AI Document Intelligence service. The file will be saved to the `./pdfs` directory with the name format `<pdf_file_name>.labels.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DocumentIntelligenceResultFormatter.save_to_labels_json(labels, pdf_file_name, pdf_labels_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain the model using the updated labels and PDF documents\n",
    "\n",
    "Once complete, the generated user feedback can be uploaded to the Azure Storage account and used to retrain the model using the Azure AI Document Intelligence service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The version of the updated model, in this example, a minor change by adding a new training document.\n",
    "updated_model_version = \"1.1.0\"\n",
    "\n",
    "# The name of the model that will be registered in Azure AI Document Intelligence\n",
    "updated_model_id = f\"{model_name}-{updated_model_version}\"\n",
    "\n",
    "# Uploads the updated user feedback documents to Azure Blob Storage and initiates model training using both the existing and new data.\n",
    "model_training_client.upload_training_data(pdf_dir)\n",
    "updated_model = model_training_client.create_model(model_name=updated_model_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
